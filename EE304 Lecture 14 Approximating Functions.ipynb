{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 304 - Neuromorphics: Brains in Silicon\n",
    "\n",
    "\n",
    "##  Approximating Functions\n",
    "\n",
    "#### The story thus far\n",
    "\n",
    "We considered how to implement synaptic weights in architectures that use shared synapses (i.e. Shared Synapse and Shared Dendrite). \n",
    "\n",
    "We found that a probabilistic approach is compatible with shared synapses. \n",
    "\n",
    "We derived expressions for the mean and variance of the total synaptic input a neuron receives in the deterministic and  probabilistic cases. \n",
    "\n",
    "We compared the memory look-ups and spike-traffic these two approaches perform and generate, respectively, to achieve a specified the signal-to-noise ratio ($r$) at the input of a neuron.\n",
    "\n",
    "We found that, for the probabilistic approach to be competitive, $r$ must be significantly less than the square-root of the number of neurons ($N$). \n",
    "\n",
    "This leads to the question: How big must $N$ be?\n",
    "\n",
    "####  Outline for this lecture\n",
    "\n",
    "<img src=\"files/lecture14/NEF+ClassicNets.pdf\" width=\"840\">\n",
    "\n",
    "We will study how the approximation to a function ($f(x)$) obtained by taking a weighted sum of tuning curves ($a_i(x)$) depends on the number of neurons $N$. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  NEF Decoders: Problem Statement \n",
    "\n",
    "<img src=\"files/lecture14/TuningCurves+f=Ad.pdf\" width=\"960\">\n",
    "\n",
    "In NEF, the decoders are obtained by solving the following problem: \n",
    "\n",
    "- Express a function $f(x)$ as a weighted sum of $N$ neural tuning curves $a_i(x)$.\n",
    "- The “decoding” weights are labeled $d_i$. \n",
    "- The function and tuning curves are sampled at $Q$ points, yielding a $Q\\times 1$ vector ${\\bf f}$ and a $Q \\times N$ matrix ${\\bf A}$, respectively. \n",
    "- The $N$ decoding weights, packed in a $N\\times 1$ vector ${\\bf d}$, are then obtained by solving the matrix equation ${\\bf f} = {\\bf Ad}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Singular Value Decomposition \n",
    "\n",
    "$$\n",
    "        {\\bf A} = \n",
    "            \\left[\\begin{array}{c|c|c|c} {\\bf u}_1 & {\\bf u}_2 & \\cdots & {\\bf u}_Q \\end{array}\\right]\n",
    "            \\left[\\begin{array}{} \n",
    "                s_1    & 0      & \\cdots & 0      \\\\\n",
    "                0      & s_2    & \\cdots & 0      \\\\\n",
    "                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                0      & 0      & \\cdots & s_N    \\\\\n",
    "                0      & 0      & \\cdots & 0      \\\\\n",
    "                \\vdots & \\vdots & \\cdots & \\vdots\n",
    "            \\end{array}\\right]\n",
    "            \\left[\\begin{array}{} {\\bf v}_1 \\\\ \\hline  {\\bf v}_2 \\\\ \\hline  \\vdots \\\\ {\\bf v}_N \\end{array}\\right]\n",
    "$$\n",
    "\n",
    "${\\bf A}$’s singular value decomposition (SVD) is:\n",
    "$$\n",
    "    {\\bf A} = {\\bf U}{\\bf S}{\\bf V}^{\\rm T} = \\sum_m^N s_m{\\bf u}_m{\\bf v}_m^{\\rm T}\n",
    "$$\n",
    "\n",
    "- ${\\bf U}$ is a $Q\\times Q$ matrix \n",
    "    - It’s columns, ${\\bf u}_m$, are an orthonormal basis for the $Q$-dimensional space\n",
    "    - ${\\bf U}^{\\rm T}{\\bf U} = {\\bf U}{\\bf U}^{\\rm T} = {\\bf I}$\n",
    "- ${\\bf S}$ is a $Q \\times N$ diagonal matrix \n",
    "    - It’s entries, ${\\bf s}_m$, are ${\\bf A}$’s singular values.\n",
    "- ${\\bf V}$ is an $N\\times N$ matrix \n",
    "    - It’s columns, ${\\bf v}_m$, are an orthonormal basis for the $N$-dimensional space\n",
    "    - ${\\bf V}^{\\rm T}{\\bf V} = {\\bf V}{\\bf V}^{\\rm T} = {\\bf I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for Decoders using SVD \n",
    "\n",
    "$$\n",
    "        {\\bf d} = \n",
    "            \\left[\\begin{array}{c|c} {\\bf v}_1 & {\\bf v}_2 & \\cdots & {\\bf v}_N \\end{array}\\right]\n",
    "            \\left[\\begin{array}{} \n",
    "                s_1^{-1} & 0        & \\cdots & 0        & 0      & \\cdots \\\\\n",
    "                0        & s_2^{-1} & \\cdots & 0        & 0      & \\cdots \\\\\n",
    "                \\vdots   & \\vdots   & \\ddots & \\vdots   & \\vdots & \\vdots \\\\\n",
    "                0        & 0        & \\cdots & s_N^{-1} & 0      & \\cdots\n",
    "            \\end{array}\\right]\n",
    "            \\left[\\begin{array}{} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_Q \\end{array}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Using SVD, we can solve for ${\\bf d}$ as follows:\n",
    "$$\n",
    "    {\\bf f} = {\\bf U}{\\bf S}{\\bf V}^{\\rm T}{\\bf d} \\implies {\\bf d} = {\\bf V}{\\bf S}^{-1}{\\bf U}^{\\rm T}{\\bf f}\n",
    "$$\n",
    "\n",
    "Writing ${\\bf f}$'s projection onto ${\\bf U}$ as \n",
    "$$\n",
    "    {\\bf c} = {\\bf U}^{\\rm T}{\\bf f} \\;{\\rm or}\\; c_q = {\\bf u}_q^{\\rm T}{\\bf f} \n",
    "$$\n",
    "\n",
    "And substituting into the expression for ${\\bf d}$ above yields\n",
    "$$\n",
    "    {\\bf d} = {\\bf V}{\\bf S}^{-1}{\\bf c} = \\sum_m^N \\frac{c_m}{s_m}{\\bf v}_m\n",
    "$$\n",
    "\n",
    "This solution is equivalent to the least-squared-error solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder's Error without Noise \n",
    "\n",
    "The root-mean-square error (RMSE) is:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    E & = & \\frac{1}{\\surd Q}\\| {\\bf f}-{\\bf Ad}\\|\\\\\n",
    "      & = & \\frac{1}{\\surd Q}\\left\\| \n",
    "            \\sum_q^Q c_q{\\bf u}_q \n",
    "            -\\left(\\sum_q^N s_q{\\bf u}_q{\\bf v}_q^{\\rm T}\\right)\n",
    "             \\left(\\sum_m^N \\frac{c_m}{s_m}{\\bf v}_m\\right)\\right\\|\\\\\n",
    "      & = & \\frac{1}{\\surd Q}\\left\\|\\sum_q^Q c_q{\\bf u}_q - \\sum_q^N c_q{\\bf u}_q\\right\\|\\\\\n",
    "      & = & \\frac{1}{\\surd Q}\\left\\|\\sum_{q=N+1}^Q c_q{\\bf u}_q\\right\\|\\\\\n",
    "      & = & \\sqrt{\\frac{1}{Q}\\sum_{q=N+1}^Q c_q^2}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Thus, error arises because $N < Q$:\n",
    "\n",
    "- $N$-dimensional neural patterns cannot fully represent a $Q$-dimensional function.\n",
    "- This prediction is quantitatively accurate ({meas, pred} above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Noise on Singular Values and Vectors\n",
    "\n",
    "<img src=\"files/lecture14/NoisySingularValues+Vectors.pdf\" width=\"960\">\n",
    "\n",
    "Benaych-Georges & Nadakuditi (2012) and Gavish & Donoho (2014) derive the asymptotic behavior of random matrixes:\n",
    "\n",
    "- As the number of rows tends to infinity (i.e., $N \\to \\infty$)\n",
    "- While keeping the ratio between numbers of rows and columns (i.e, $N/Q$) constant \n",
    "\n",
    "Their theory may be used to relate the noisy measurment matrix's ($\\tilde{\\bf A}$) singular values and vectors to those of the noiseless matrix (${\\bf A}$).\n",
    "\n",
    "It yields expressions that relate:\n",
    "\n",
    "- The new singular values to the old ones (left panel)\n",
    "    - Noise boosts small values, shrinking the overall range\n",
    "- The inner product between a new and an old singular vector to the old singular value (middle and right panels)\n",
    "    - It becomes zero when the old singular-value drops below a cut-off \n",
    "    - The red line in the left panel indicates this cut-off\n",
    "\n",
    "The examples shown, and those in the rest of this presentation, use square-root tuning curves (see Slide 3) with N = 200, Q = 400, and σ = 0.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymptotic Analytical Results \n",
    "\n",
    "<img src=\"files/lecture14/SingularValueNoiselessToNoisy.pdf\" width=\"480\">\n",
    "\n",
    "In the case where normally-distributed noise (with variance $\\sigma^2$ and mean zero) is added to the entries of ${\\bf A}$ to obtain $\\tilde{\\bf A}$:\n",
    "\n",
    "- $\\tilde{\\bf A}$'s singular values are related to ${\\bf A}$'s by\n",
    "\n",
    "$$\n",
    "    \\tilde{s}_q = s_q\\sqrt{\\left(1+Q\\sigma^2/s_q^2\\right)\\left(1+N\\sigma^2/s_q^2\\right)}\n",
    "$$\n",
    "\n",
    "- And the inner products of their singular vectors are\n",
    "\n",
    "$$\n",
    "    {\\bf u}^{\\rm T}{\\bf u}=\\sqrt{\\frac{1-NQ\\sigma^4/s_q^4}{1+Q\\sigma^2/s_q^2}}\n",
    "    \\;\\;{\\rm and}\\;\\;\n",
    "    {\\bf v}^{\\rm T}{\\bf v}=\\sqrt{\\frac{1-NQ\\sigma^4/s_q^4}{1+N\\sigma^2/s_q^2}}\n",
    "$$\n",
    "\n",
    "- if $s_q^2/\\sigma^2>\\sqrt{NQ}$. Otherwise, the inner product is zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder's Error with Noise\n",
    "\n",
    "The RMSE on a test set $\\hat{A}=\\hat{\\bf U}\\hat{\\bf S}\\hat{\\bf V}^{\\rm T}$ is:\n",
    "\\begin{eqnarray*}\n",
    "    E & = & \\frac{1}{\\surd Q}\\| {\\bf f}-{\\bf Ad}\\|\\\\\n",
    "      & = & \\frac{1}{\\surd Q}\\left\\| \n",
    "            \\sum_q^Q c_q{\\bf u}_q \n",
    "            -\\left(\\sum_q^N \\hat{s}_q\\hat{\\bf u}_q\\hat{\\bf v}_q^{\\rm T}\\right)\n",
    "             \\left(\\sum_m^N \\frac{c_m}{s_m}{\\bf v}_m\\right)\\right\\|\\\\\n",
    "      & = & \\frac{1}{\\surd Q}\\left\\|\\sum_q^Q c_q{\\bf u}_q \n",
    "            - \\sum_q^r \\frac{\\hat{s}_q}{s_q}c_q\\hat{\\bf u}_q\\hat{\\bf v}_q^{\\rm T}{\\bf v}_q\\right\\|\\\\\n",
    "      & = & \\frac{1}{\\surd Q}\\left\\|\n",
    "              \\sum_{q=N+1}^Q c_q\\left({\\bf u}_q-\\frac{\\hat{s}_q}{s_q}\\hat{\\bf v}_q^{\\rm T}{\\bf v}_q\\hat{\\bf u}_q\\right)\n",
    "              + \\sum_{q=r+1}^Q c_q{\\bf u}_q\\right\\|\\\\\n",
    "      & = & \\frac{1}{\\surd Q}\n",
    "            \\sqrt{\n",
    "              \\sum_q^r \\left(\n",
    "                  1 + \\left(\\frac{\\hat{s}_q}{s_q}\\hat{\\bf v}_q^{\\rm T}{\\bf v}_q\\right)^2 \n",
    "                  - 2\\frac{\\hat{s}_q}{s_q}\\hat{\\bf v}_q^{\\rm T}{\\bf v}_q\\hat{\\bf u}_q^{\\rm T}{\\bf u}_q\n",
    "                  \\right)c_q^2\n",
    "              + \\sum_{q=r+1}^Q c_q^2\n",
    "              }\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
