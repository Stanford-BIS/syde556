{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 304 - Neuromorphics: Brains in Silicon\n",
    "\n",
    "\n",
    "##  Neuromorphic Architectures\n",
    "\n",
    "####  The story thus far\n",
    "\n",
    "We have implemented two of a neuron's elements:\n",
    "\n",
    "- An exponentially decaying **synapse**\n",
    "    - a log-domain lowpass filter\n",
    "- A quadratic integrate-and-fire **soma**\n",
    "    - a log-domain lowpass filter plus a current mirror\n",
    "\n",
    "In this lecture, we will start looking at architectures for interconnecting these elements in a neuromorphic chip to implement spiking neural networks. \n",
    "\n",
    "#### Dedicating versus sharing hardware elements\n",
    "\n",
    "When we implement networks of spiking neurons in silicon, we may choose to:\n",
    "\n",
    "- <b> Dedicate </b> each hardware element to a single neuronal element\n",
    "    - the neural elements are: \n",
    "        - Axon\n",
    "        - Synapse\n",
    "        - Dendrite\n",
    "        - Soma\n",
    "- <b> Share </b> it among several neural elements\n",
    "    - the resulting architectures are: \n",
    "        - Fully Dedicated\n",
    "        - Shared Axon\n",
    "        - Shared Synapse\n",
    "        - Shared Dendrite \n",
    "\n",
    "#### Resulting architectures have different scaling properties\n",
    "\n",
    "For instance, in the Shared-Axon Architecture:\n",
    "\n",
    "- Instead of carrying a silicon neuron's spikes on a metal wire dedicated to it\n",
    "- We transmit its spike train on a bus\n",
    "- Along with spike trains of other silicon neurons\n",
    "- This cuts the number of wires from $N$ to $\\log_2(N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Four Neuronal Elements \n",
    "\n",
    "<img src=\"files/lecture10/TwoNeuronNetwork.png\" width=\"720\">\n",
    "\n",
    "In all, a neuron has four elements:\n",
    "\n",
    "- <b>Axon</b>: Communicates its spikes to other neurons\n",
    "- <b>Synapse</b>: Coverts a spike to a graded potential\n",
    "- <b>Dendrite</b>: Summates these graded potentials \n",
    "- <b>Soma</b>: Converts these summated graded potentials to a spike train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Four Architectures \n",
    "\n",
    "As we map a spiking neural network's neuronal elements onto our silicon chip's hardware elements, we must choose whether to dedicate or share them.\n",
    "\n",
    "We may make choices, progressively, for each of the four types of neuronal elements.\n",
    "\n",
    "This leads to four distinct architectures:\n",
    "\n",
    "- <b>Fully-Dedicated</b>: Each hardware element is dedicated to a single neuronal element\n",
    "- <b>Shared-Axon</b>: $\\log_2(N)$ metal wires are shared by $N$ axons \n",
    "- <b>Shared-Synapse</b>: A single lowpass-filter is shared by all of a neuron's $N$ synapses\n",
    "- <b>Shared-Dendrite</b>: A single resistive-mesh is shared by all $N$ neurons' dendrites\n",
    "\n",
    "As we move down the line, sharing more and more types of elements, the hardware savings compound:\n",
    "\n",
    "<img src=\"files/lecture10/ArchitecturesSynRAMScaling.png\" width=\"720\">\n",
    "$A$ is the number of neurons a dendritic arbor spans\n",
    "\n",
    "- Synapse-circuit count drops from $N^2$ to $N/A$\n",
    "- RAM words needed to store weights drops from $N^2$ to $N^2/A$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Fully-Dedicated Architecture\n",
    "\n",
    "<img src=\"files/lecture10/FullyDedicated.png\" width=\"360\">\n",
    "\n",
    "Requires $N^2$ synapse circuits to fully connect $N$ neurons:\n",
    "\n",
    "- No hardware elements are shared\n",
    "    - Correspondence between harware elements and neural elements is one-to-one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared-Axon Architecture\n",
    "\n",
    "<img src=\"files/lecture10/SharedAxon.png\" width=\"360\">\n",
    "\n",
    "$\\log_2(N)$ wires are shared by $N$ axons:\n",
    "\n",
    "- Communicates spikes as addresses \n",
    "- Each neuron is assigned a unique addess\n",
    "- This representation is called an **address-event**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared-Synapse Architecture\n",
    "\n",
    "<img src=\"files/lecture10/SharedSynapse.png\" width=\"440\">\n",
    "\n",
    "Uses only $N$ synapse circuits to fully connect $N$ neurons:\n",
    "\n",
    "- A single circuit models each neuron’s $N$ synapses\n",
    "- All spikes destined for these synapses are routed to it\n",
    "- Requires a external RAM to store weights\n",
    "\n",
    "% Do convolution in time\n",
    "\n",
    "### Superposable Synapse Circuit for Shared Synapse\n",
    "\n",
    "<img src=\"files/lecture10/SuperposableSynapse.svg\" width=\"840\">\n",
    "\n",
    "One synapse circuit replaces three:\n",
    "- It receives their three input spike trains, combined into a single spike-train\n",
    "- It **linearly superposes** the three outputs they would have produced \n",
    "- It assumes that all three synapses' had the **same weight**\n",
    "    - All spikes are weighted equally\n",
    "\n",
    "This works because the circuit's input-output relationship is described by\n",
    "\n",
    "$$ \n",
    "    v(t) = \\int_{-\\infty}^{t} \\!\\! h(t-\\tau)\\sum_i \\delta(\\tau-t_i) dt \n",
    "         = \\sum_i h(t-t_i) \n",
    "         \\;\\; {\\rm with} \\;\\; \n",
    "    h(t) = {1 \\over \\tau}u(t)e^{-t/\\tau} \n",
    "$$\n",
    "- A convolution with a kernel that **decays exponentially** with time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared-Dendrite\n",
    "\n",
    "<img src=\"files/lecture10/SharedDendrite.png\" width=\"480\">\n",
    "\n",
    "Ratio of shared-synapse circuits to neurons drops below one-to-one:\n",
    "\n",
    "- A resistive mesh models exponential decay along dendrites\n",
    "- A single, shared, resistive mesh models all $N$ neurons’ dendritic trees\n",
    "- This mesh is implemented with transistors\n",
    "- Neurons receive input from neighboring shared-synapse circuits\n",
    "\n",
    "The RAM’s size and bandwidth are cut as well\n",
    "\n",
    "### Diffusor Circuit: Transistor-Based Resistive Mesh\n",
    "\n",
    "<img src=\"files/lecture10/diffusor.svg\" width=\"960\">\n",
    "\n",
    "This transistor-based implementation is perfectly linear:\n",
    "- In the subthreshold current-domain \n",
    "- Over three to four decades (see plot)\n",
    "\n",
    "In the continuum limit, its current-in-current-out relationship is given by \n",
    "$$ I^(x) = \\int_{-\\infty}^{+\\infty} \\!\\! k(x-u)I^*(u)du \\;\\; {\\rm with} \\;\\; k(x) = \\frac{1}{2L}e^{-x/L}  $$\n",
    "- A convolution with a kernel that **decays exponentially** with distance\n",
    "- The space-constant $L \\approx \\exp(\\kappa(V_c - V_r)/2U_T)$\n",
    "\n",
    "We can define $L$ as the dendritic **arbor's radius**\n",
    "- The current received decays by are factor of $e$ \n",
    "- Neurons within this distance receive significant input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Shared\n",
    "\n",
    "<img src=\"files/lecture10/FullyShared.png\" width=\"540\">\n",
    "\n",
    "A single, shared arithmetic unit models all elements of the neural network:\n",
    "\n",
    "- Each time a spike occurs, it updates the membrane voltages of that neuron’s targets\n",
    "- It retrieves the old value as well as the synaptic weights from RAM\n",
    "- If the new value exceeds threshold, it issues a spike\n",
    "    - Adds it to the address-event queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### In Summary\n",
    "\n",
    "<img src=\"files/lecture10/ArchitecturesSynRAMScaling.png\" width=\"720\">\n",
    "$A$ is the number of neurons a dendritic arbor spans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
